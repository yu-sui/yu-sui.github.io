---
title: "About"
date: 2025-12-16T17:57:49-08:00
draft: false
---

Hi, I’m **Yu Sui**, a software / hardware co-design engineer working on performance-critical systems.

My professional background is rooted in FPGA-based system development for large-scale digital circuit emulation and prototyping. Over the years, my work has spanned multiple layers of the stack—from low-level software and runtime systems to RTL design and full FPGA platform bring-up.

Early in my career, I worked extensively on **Linux-based high-performance C++ systems**, focusing on runtime design, multi-threaded programming, and low-level performance optimization. This experience shaped how I reason about system behavior, resource utilization, and bottlenecks at the software level.

I later transitioned into **FPGA and RTL-centric development**, where my work involved:
- RTL and interface design, with a strong focus on AXI-based and custom on-chip protocols  
- High-speed FPGA IP bring-up, including SerDes, DDR, and related infrastructure  
- Full-system bring-up of heterogeneous platforms combining processing systems (PS) and programmable logic (PL)

In parallel, I have maintained a long-standing interest in **embedded and low-power systems**. During my PhD studies, I worked on low-power microsystems for oil well sensing, and I continue to explore microcontroller-based systems (e.g., low-power Silicon Labs MCUs) as a way to stay grounded in energy-efficient design and system-level trade-offs.

### Current Interests
More recently, my focus has shifted toward **LLM inference infrastructure**, particularly the design and optimization of efficient inference systems.

I am currently spending most of my time working with and studying projects such as **vLLM** and **llama.cpp**, with an emphasis on understanding their internal architecture, performance characteristics, and system-level trade-offs.

My interests center around:
- Inference system architecture and runtime design  
- Memory management, scheduling, and parallelism in large model inference  
- Performance analysis and optimization for LLM serving workloads  
- The interaction between inference software stacks and underlying hardware capabilities  

While my background includes extensive experience with FPGA and low-level hardware programming, my current focus is on building and reasoning about **robust, efficient inference infrastructure**, with hardware acceleration (GPU or FPGA) treated as an enabling layer rather than the primary abstraction.

### About This Site
This site serves as a collection of technical notes and engineering reflections. Rather than presenting polished tutorials or product-specific details, the focus is on:
- Design trade-offs and architectural reasoning  
- Performance analysis and optimization strategies  
- Practical insights from building and debugging complex systems  

Most posts are written as engineering notes or design reviews, with an emphasis on clarity, constraints, and reproducibility.

### Links
- Google Scholar: https://scholar.google.com/citations?user=Youm7ZMAAAAJ&hl=en
- GitHub: https://github.com/yu-sui
- Email: sui@umich.edu
